# algo 
algo: "RecurrentPPO"
total_timesteps: 6000000
algo_config:
    tensorboard_log: "../../../logs"
    #
    policy: 'MlpLstmPolicy'
    normalize_advantage: True
    batch_size: 256
    n_steps: 1024
    gamma: 0.9999
    learning_rate: !!float 7.77e-05
    ent_coef: 0.00429
    clip_range: 0.1
    n_epochs: 10
    gae_lambda: 0.9
    max_grad_norm: 5
    vf_coef: 0.19
    use_sde: True
    sde_sample_freq: 8
    policy_kwargs: "dict(log_std_init=0.0, ortho_init=False,
                       lstm_hidden_size=16,
                       enable_critic_lstm = True,
                       net_arch=[64, 32, 16])"

# other lstm options:
#
# enable_critic_lstm = False (seems to not work)
# n_lstm_layers >= 1 (default 1)
# feature_extractor_class: e.g. stable_baselines3.common.torch_layers:MlpExtractor
#     - feature_extractor_kwargs
#

# env
env_id: "AsmEnv"
config: 
    observation_fn_id: 'observe_1o'
    n_observs: 1
    obs_noise: 0.2
    #
    harvest_fn_name: "default"
    upow: 1
n_envs: 12

# io
repo: "cboettig/rl-ecology"
save_path: "../saved_agents/results/"

# misc
id: "1obs-UM1-lstm-64-32-16"
# id: "short-test"
additional_imports: ["torch"]