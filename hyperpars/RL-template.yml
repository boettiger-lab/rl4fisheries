# algo 
algo: "PPO" # Proximal Policy Optimization
total_timesteps: 6000000 # training steps
algo_config:
    policy: 'MlpPolicy' # 'multilayer perceptron' aka a 'feedforward' network type
    policy_kwargs: "dict(net_arch=[64, 32, 16])" # network structure - three layers 64-32-16
    use_sde: True

# name with which we register Walleye dynamical model 
# (see src/rl4fisheries/__init__.py for registration)
env_id: "AsmEnv" 

# configuration dictionary passed as argument to AsmEnv
config: 
    observation_fn_id: 'observe_1o'
    n_observs: 1
    obs_noise: 0.1
    #
    harvest_fn_name: "default"
    upow: 1

# paralelizing training for speed
n_envs: 12

# save and upload trained model
repo: "cboettig/rl-ecology" # huggingface repository
save_path: "../saved_agents/results/"

# misc
id: "biomass-UM1-64-32-16-noise0.1"
additional_imports: ["torch"] # needed for custom network structure